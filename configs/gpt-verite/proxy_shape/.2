# This is a base shape file encoded in yaml
# - `null` indicates a dimension is "finite", i.e. a non-"width" dimension
# - a number indicates the base dimension of an "infinite" dimension, i.e. some notion of "width"
sequential.0.word_embeddings.weight:
- null
- 256
sequential.10.attention.dense.bias:
- 256
sequential.10.attention.dense.weight:
- 256
- 256
sequential.10.attention.query_key_value.bias:
- 768
sequential.10.attention.query_key_value.weight:
- 768
- 256
sequential.10.input_layernorm.bias:
- 256
sequential.10.input_layernorm.weight:
- 256
sequential.10.mlp.dense_4h_to_h.bias:
- 256
sequential.10.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.10.mlp.dense_h_to_4h.bias:
- 1024
sequential.10.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.10.post_attention_layernorm.bias:
- 256
sequential.10.post_attention_layernorm.weight:
- 256
sequential.11.attention.dense.bias:
- 256
sequential.11.attention.dense.weight:
- 256
- 256
sequential.11.attention.query_key_value.bias:
- 768
sequential.11.attention.query_key_value.weight:
- 768
- 256
sequential.11.input_layernorm.bias:
- 256
sequential.11.input_layernorm.weight:
- 256
sequential.11.mlp.dense_4h_to_h.bias:
- 256
sequential.11.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.11.mlp.dense_h_to_4h.bias:
- 1024
sequential.11.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.11.post_attention_layernorm.bias:
- 256
sequential.11.post_attention_layernorm.weight:
- 256
sequential.12.attention.dense.bias:
- 256
sequential.12.attention.dense.weight:
- 256
- 256
sequential.12.attention.query_key_value.bias:
- 768
sequential.12.attention.query_key_value.weight:
- 768
- 256
sequential.12.input_layernorm.bias:
- 256
sequential.12.input_layernorm.weight:
- 256
sequential.12.mlp.dense_4h_to_h.bias:
- 256
sequential.12.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.12.mlp.dense_h_to_4h.bias:
- 1024
sequential.12.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.12.post_attention_layernorm.bias:
- 256
sequential.12.post_attention_layernorm.weight:
- 256
sequential.13.attention.dense.bias:
- 256
sequential.13.attention.dense.weight:
- 256
- 256
sequential.13.attention.query_key_value.bias:
- 768
sequential.13.attention.query_key_value.weight:
- 768
- 256
sequential.13.input_layernorm.bias:
- 256
sequential.13.input_layernorm.weight:
- 256
sequential.13.mlp.dense_4h_to_h.bias:
- 256
sequential.13.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.13.mlp.dense_h_to_4h.bias:
- 1024
sequential.13.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.13.post_attention_layernorm.bias:
- 256
sequential.13.post_attention_layernorm.weight:
- 256
sequential.14.attention.dense.bias:
- 256
sequential.14.attention.dense.weight:
- 256
- 256
sequential.14.attention.query_key_value.bias:
- 768
sequential.14.attention.query_key_value.weight:
- 768
- 256
sequential.14.input_layernorm.bias:
- 256
sequential.14.input_layernorm.weight:
- 256
sequential.14.mlp.dense_4h_to_h.bias:
- 256
sequential.14.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.14.mlp.dense_h_to_4h.bias:
- 1024
sequential.14.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.14.post_attention_layernorm.bias:
- 256
sequential.14.post_attention_layernorm.weight:
- 256
sequential.15.attention.dense.bias:
- 256
sequential.15.attention.dense.weight:
- 256
- 256
sequential.15.attention.query_key_value.bias:
- 768
sequential.15.attention.query_key_value.weight:
- 768
- 256
sequential.15.input_layernorm.bias:
- 256
sequential.15.input_layernorm.weight:
- 256
sequential.15.mlp.dense_4h_to_h.bias:
- 256
sequential.15.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.15.mlp.dense_h_to_4h.bias:
- 1024
sequential.15.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.15.post_attention_layernorm.bias:
- 256
sequential.15.post_attention_layernorm.weight:
- 256
sequential.16.attention.dense.bias:
- 256
sequential.16.attention.dense.weight:
- 256
- 256
sequential.16.attention.query_key_value.bias:
- 768
sequential.16.attention.query_key_value.weight:
- 768
- 256
sequential.16.input_layernorm.bias:
- 256
sequential.16.input_layernorm.weight:
- 256
sequential.16.mlp.dense_4h_to_h.bias:
- 256
sequential.16.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.16.mlp.dense_h_to_4h.bias:
- 1024
sequential.16.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.16.post_attention_layernorm.bias:
- 256
sequential.16.post_attention_layernorm.weight:
- 256
sequential.17.attention.dense.bias:
- 256
sequential.17.attention.dense.weight:
- 256
- 256
sequential.17.attention.query_key_value.bias:
- 768
sequential.17.attention.query_key_value.weight:
- 768
- 256
sequential.17.input_layernorm.bias:
- 256
sequential.17.input_layernorm.weight:
- 256
sequential.17.mlp.dense_4h_to_h.bias:
- 256
sequential.17.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.17.mlp.dense_h_to_4h.bias:
- 1024
sequential.17.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.17.post_attention_layernorm.bias:
- 256
sequential.17.post_attention_layernorm.weight:
- 256
sequential.18.attention.dense.bias:
- 256
sequential.18.attention.dense.weight:
- 256
- 256
sequential.18.attention.query_key_value.bias:
- 768
sequential.18.attention.query_key_value.weight:
- 768
- 256
sequential.18.input_layernorm.bias:
- 256
sequential.18.input_layernorm.weight:
- 256
sequential.18.mlp.dense_4h_to_h.bias:
- 256
sequential.18.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.18.mlp.dense_h_to_4h.bias:
- 1024
sequential.18.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.18.post_attention_layernorm.bias:
- 256
sequential.18.post_attention_layernorm.weight:
- 256
sequential.19.attention.dense.bias:
- 256
sequential.19.attention.dense.weight:
- 256
- 256
sequential.19.attention.query_key_value.bias:
- 768
sequential.19.attention.query_key_value.weight:
- 768
- 256
sequential.19.input_layernorm.bias:
- 256
sequential.19.input_layernorm.weight:
- 256
sequential.19.mlp.dense_4h_to_h.bias:
- 256
sequential.19.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.19.mlp.dense_h_to_4h.bias:
- 1024
sequential.19.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.19.post_attention_layernorm.bias:
- 256
sequential.19.post_attention_layernorm.weight:
- 256
sequential.2.attention.dense.bias:
- 256
sequential.2.attention.dense.weight:
- 256
- 256
sequential.2.attention.query_key_value.bias:
- 768
sequential.2.attention.query_key_value.weight:
- 768
- 256
sequential.2.input_layernorm.bias:
- 256
sequential.2.input_layernorm.weight:
- 256
sequential.2.mlp.dense_4h_to_h.bias:
- 256
sequential.2.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.2.mlp.dense_h_to_4h.bias:
- 1024
sequential.2.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.2.post_attention_layernorm.bias:
- 256
sequential.2.post_attention_layernorm.weight:
- 256
sequential.20.attention.dense.bias:
- 256
sequential.20.attention.dense.weight:
- 256
- 256
sequential.20.attention.query_key_value.bias:
- 768
sequential.20.attention.query_key_value.weight:
- 768
- 256
sequential.20.input_layernorm.bias:
- 256
sequential.20.input_layernorm.weight:
- 256
sequential.20.mlp.dense_4h_to_h.bias:
- 256
sequential.20.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.20.mlp.dense_h_to_4h.bias:
- 1024
sequential.20.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.20.post_attention_layernorm.bias:
- 256
sequential.20.post_attention_layernorm.weight:
- 256
sequential.21.attention.dense.bias:
- 256
sequential.21.attention.dense.weight:
- 256
- 256
sequential.21.attention.query_key_value.bias:
- 768
sequential.21.attention.query_key_value.weight:
- 768
- 256
sequential.21.input_layernorm.bias:
- 256
sequential.21.input_layernorm.weight:
- 256
sequential.21.mlp.dense_4h_to_h.bias:
- 256
sequential.21.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.21.mlp.dense_h_to_4h.bias:
- 1024
sequential.21.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.21.post_attention_layernorm.bias:
- 256
sequential.21.post_attention_layernorm.weight:
- 256
sequential.22.attention.dense.bias:
- 256
sequential.22.attention.dense.weight:
- 256
- 256
sequential.22.attention.query_key_value.bias:
- 768
sequential.22.attention.query_key_value.weight:
- 768
- 256
sequential.22.input_layernorm.bias:
- 256
sequential.22.input_layernorm.weight:
- 256
sequential.22.mlp.dense_4h_to_h.bias:
- 256
sequential.22.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.22.mlp.dense_h_to_4h.bias:
- 1024
sequential.22.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.22.post_attention_layernorm.bias:
- 256
sequential.22.post_attention_layernorm.weight:
- 256
sequential.23.attention.dense.bias:
- 256
sequential.23.attention.dense.weight:
- 256
- 256
sequential.23.attention.query_key_value.bias:
- 768
sequential.23.attention.query_key_value.weight:
- 768
- 256
sequential.23.input_layernorm.bias:
- 256
sequential.23.input_layernorm.weight:
- 256
sequential.23.mlp.dense_4h_to_h.bias:
- 256
sequential.23.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.23.mlp.dense_h_to_4h.bias:
- 1024
sequential.23.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.23.post_attention_layernorm.bias:
- 256
sequential.23.post_attention_layernorm.weight:
- 256
sequential.24.attention.dense.bias:
- 256
sequential.24.attention.dense.weight:
- 256
- 256
sequential.24.attention.query_key_value.bias:
- 768
sequential.24.attention.query_key_value.weight:
- 768
- 256
sequential.24.input_layernorm.bias:
- 256
sequential.24.input_layernorm.weight:
- 256
sequential.24.mlp.dense_4h_to_h.bias:
- 256
sequential.24.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.24.mlp.dense_h_to_4h.bias:
- 1024
sequential.24.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.24.post_attention_layernorm.bias:
- 256
sequential.24.post_attention_layernorm.weight:
- 256
sequential.25.attention.dense.bias:
- 256
sequential.25.attention.dense.weight:
- 256
- 256
sequential.25.attention.query_key_value.bias:
- 768
sequential.25.attention.query_key_value.weight:
- 768
- 256
sequential.25.input_layernorm.bias:
- 256
sequential.25.input_layernorm.weight:
- 256
sequential.25.mlp.dense_4h_to_h.bias:
- 256
sequential.25.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.25.mlp.dense_h_to_4h.bias:
- 1024
sequential.25.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.25.post_attention_layernorm.bias:
- 256
sequential.25.post_attention_layernorm.weight:
- 256
sequential.26.attention.dense.bias:
- 256
sequential.26.attention.dense.weight:
- 256
- 256
sequential.26.attention.query_key_value.bias:
- 768
sequential.26.attention.query_key_value.weight:
- 768
- 256
sequential.26.input_layernorm.bias:
- 256
sequential.26.input_layernorm.weight:
- 256
sequential.26.mlp.dense_4h_to_h.bias:
- 256
sequential.26.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.26.mlp.dense_h_to_4h.bias:
- 1024
sequential.26.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.26.post_attention_layernorm.bias:
- 256
sequential.26.post_attention_layernorm.weight:
- 256
sequential.27.attention.dense.bias:
- 256
sequential.27.attention.dense.weight:
- 256
- 256
sequential.27.attention.query_key_value.bias:
- 768
sequential.27.attention.query_key_value.weight:
- 768
- 256
sequential.27.input_layernorm.bias:
- 256
sequential.27.input_layernorm.weight:
- 256
sequential.27.mlp.dense_4h_to_h.bias:
- 256
sequential.27.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.27.mlp.dense_h_to_4h.bias:
- 1024
sequential.27.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.27.post_attention_layernorm.bias:
- 256
sequential.27.post_attention_layernorm.weight:
- 256
sequential.28.attention.dense.bias:
- 256
sequential.28.attention.dense.weight:
- 256
- 256
sequential.28.attention.query_key_value.bias:
- 768
sequential.28.attention.query_key_value.weight:
- 768
- 256
sequential.28.input_layernorm.bias:
- 256
sequential.28.input_layernorm.weight:
- 256
sequential.28.mlp.dense_4h_to_h.bias:
- 256
sequential.28.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.28.mlp.dense_h_to_4h.bias:
- 1024
sequential.28.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.28.post_attention_layernorm.bias:
- 256
sequential.28.post_attention_layernorm.weight:
- 256
sequential.29.attention.dense.bias:
- 256
sequential.29.attention.dense.weight:
- 256
- 256
sequential.29.attention.query_key_value.bias:
- 768
sequential.29.attention.query_key_value.weight:
- 768
- 256
sequential.29.input_layernorm.bias:
- 256
sequential.29.input_layernorm.weight:
- 256
sequential.29.mlp.dense_4h_to_h.bias:
- 256
sequential.29.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.29.mlp.dense_h_to_4h.bias:
- 1024
sequential.29.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.29.post_attention_layernorm.bias:
- 256
sequential.29.post_attention_layernorm.weight:
- 256
sequential.3.attention.dense.bias:
- 256
sequential.3.attention.dense.weight:
- 256
- 256
sequential.3.attention.query_key_value.bias:
- 768
sequential.3.attention.query_key_value.weight:
- 768
- 256
sequential.3.input_layernorm.bias:
- 256
sequential.3.input_layernorm.weight:
- 256
sequential.3.mlp.dense_4h_to_h.bias:
- 256
sequential.3.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.3.mlp.dense_h_to_4h.bias:
- 1024
sequential.3.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.3.post_attention_layernorm.bias:
- 256
sequential.3.post_attention_layernorm.weight:
- 256
sequential.30.attention.dense.bias:
- 256
sequential.30.attention.dense.weight:
- 256
- 256
sequential.30.attention.query_key_value.bias:
- 768
sequential.30.attention.query_key_value.weight:
- 768
- 256
sequential.30.input_layernorm.bias:
- 256
sequential.30.input_layernorm.weight:
- 256
sequential.30.mlp.dense_4h_to_h.bias:
- 256
sequential.30.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.30.mlp.dense_h_to_4h.bias:
- 1024
sequential.30.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.30.post_attention_layernorm.bias:
- 256
sequential.30.post_attention_layernorm.weight:
- 256
sequential.31.attention.dense.bias:
- 256
sequential.31.attention.dense.weight:
- 256
- 256
sequential.31.attention.query_key_value.bias:
- 768
sequential.31.attention.query_key_value.weight:
- 768
- 256
sequential.31.input_layernorm.bias:
- 256
sequential.31.input_layernorm.weight:
- 256
sequential.31.mlp.dense_4h_to_h.bias:
- 256
sequential.31.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.31.mlp.dense_h_to_4h.bias:
- 1024
sequential.31.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.31.post_attention_layernorm.bias:
- 256
sequential.31.post_attention_layernorm.weight:
- 256
sequential.32.attention.dense.bias:
- 256
sequential.32.attention.dense.weight:
- 256
- 256
sequential.32.attention.query_key_value.bias:
- 768
sequential.32.attention.query_key_value.weight:
- 768
- 256
sequential.32.input_layernorm.bias:
- 256
sequential.32.input_layernorm.weight:
- 256
sequential.32.mlp.dense_4h_to_h.bias:
- 256
sequential.32.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.32.mlp.dense_h_to_4h.bias:
- 1024
sequential.32.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.32.post_attention_layernorm.bias:
- 256
sequential.32.post_attention_layernorm.weight:
- 256
sequential.33.attention.dense.bias:
- 256
sequential.33.attention.dense.weight:
- 256
- 256
sequential.33.attention.query_key_value.bias:
- 768
sequential.33.attention.query_key_value.weight:
- 768
- 256
sequential.33.input_layernorm.bias:
- 256
sequential.33.input_layernorm.weight:
- 256
sequential.33.mlp.dense_4h_to_h.bias:
- 256
sequential.33.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.33.mlp.dense_h_to_4h.bias:
- 1024
sequential.33.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.33.post_attention_layernorm.bias:
- 256
sequential.33.post_attention_layernorm.weight:
- 256
sequential.34.attention.dense.bias:
- 256
sequential.34.attention.dense.weight:
- 256
- 256
sequential.34.attention.query_key_value.bias:
- 768
sequential.34.attention.query_key_value.weight:
- 768
- 256
sequential.34.input_layernorm.bias:
- 256
sequential.34.input_layernorm.weight:
- 256
sequential.34.mlp.dense_4h_to_h.bias:
- 256
sequential.34.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.34.mlp.dense_h_to_4h.bias:
- 1024
sequential.34.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.34.post_attention_layernorm.bias:
- 256
sequential.34.post_attention_layernorm.weight:
- 256
sequential.35.attention.dense.bias:
- 256
sequential.35.attention.dense.weight:
- 256
- 256
sequential.35.attention.query_key_value.bias:
- 768
sequential.35.attention.query_key_value.weight:
- 768
- 256
sequential.35.input_layernorm.bias:
- 256
sequential.35.input_layernorm.weight:
- 256
sequential.35.mlp.dense_4h_to_h.bias:
- 256
sequential.35.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.35.mlp.dense_h_to_4h.bias:
- 1024
sequential.35.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.35.post_attention_layernorm.bias:
- 256
sequential.35.post_attention_layernorm.weight:
- 256
sequential.36.attention.dense.bias:
- 256
sequential.36.attention.dense.weight:
- 256
- 256
sequential.36.attention.query_key_value.bias:
- 768
sequential.36.attention.query_key_value.weight:
- 768
- 256
sequential.36.input_layernorm.bias:
- 256
sequential.36.input_layernorm.weight:
- 256
sequential.36.mlp.dense_4h_to_h.bias:
- 256
sequential.36.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.36.mlp.dense_h_to_4h.bias:
- 1024
sequential.36.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.36.post_attention_layernorm.bias:
- 256
sequential.36.post_attention_layernorm.weight:
- 256
sequential.37.attention.dense.bias:
- 256
sequential.37.attention.dense.weight:
- 256
- 256
sequential.37.attention.query_key_value.bias:
- 768
sequential.37.attention.query_key_value.weight:
- 768
- 256
sequential.37.input_layernorm.bias:
- 256
sequential.37.input_layernorm.weight:
- 256
sequential.37.mlp.dense_4h_to_h.bias:
- 256
sequential.37.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.37.mlp.dense_h_to_4h.bias:
- 1024
sequential.37.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.37.post_attention_layernorm.bias:
- 256
sequential.37.post_attention_layernorm.weight:
- 256
sequential.38.attention.dense.bias:
- 256
sequential.38.attention.dense.weight:
- 256
- 256
sequential.38.attention.query_key_value.bias:
- 768
sequential.38.attention.query_key_value.weight:
- 768
- 256
sequential.38.input_layernorm.bias:
- 256
sequential.38.input_layernorm.weight:
- 256
sequential.38.mlp.dense_4h_to_h.bias:
- 256
sequential.38.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.38.mlp.dense_h_to_4h.bias:
- 1024
sequential.38.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.38.post_attention_layernorm.bias:
- 256
sequential.38.post_attention_layernorm.weight:
- 256
sequential.39.attention.dense.bias:
- 256
sequential.39.attention.dense.weight:
- 256
- 256
sequential.39.attention.query_key_value.bias:
- 768
sequential.39.attention.query_key_value.weight:
- 768
- 256
sequential.39.input_layernorm.bias:
- 256
sequential.39.input_layernorm.weight:
- 256
sequential.39.mlp.dense_4h_to_h.bias:
- 256
sequential.39.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.39.mlp.dense_h_to_4h.bias:
- 1024
sequential.39.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.39.post_attention_layernorm.bias:
- 256
sequential.39.post_attention_layernorm.weight:
- 256
sequential.4.attention.dense.bias:
- 256
sequential.4.attention.dense.weight:
- 256
- 256
sequential.4.attention.query_key_value.bias:
- 768
sequential.4.attention.query_key_value.weight:
- 768
- 256
sequential.4.input_layernorm.bias:
- 256
sequential.4.input_layernorm.weight:
- 256
sequential.4.mlp.dense_4h_to_h.bias:
- 256
sequential.4.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.4.mlp.dense_h_to_4h.bias:
- 1024
sequential.4.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.4.post_attention_layernorm.bias:
- 256
sequential.4.post_attention_layernorm.weight:
- 256
sequential.41.norm.bias:
- 256
sequential.41.norm.weight:
- 256
sequential.42.final_linear.weight:
- null
- 256
sequential.5.attention.dense.bias:
- 256
sequential.5.attention.dense.weight:
- 256
- 256
sequential.5.attention.query_key_value.bias:
- 768
sequential.5.attention.query_key_value.weight:
- 768
- 256
sequential.5.input_layernorm.bias:
- 256
sequential.5.input_layernorm.weight:
- 256
sequential.5.mlp.dense_4h_to_h.bias:
- 256
sequential.5.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.5.mlp.dense_h_to_4h.bias:
- 1024
sequential.5.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.5.post_attention_layernorm.bias:
- 256
sequential.5.post_attention_layernorm.weight:
- 256
sequential.6.attention.dense.bias:
- 256
sequential.6.attention.dense.weight:
- 256
- 256
sequential.6.attention.query_key_value.bias:
- 768
sequential.6.attention.query_key_value.weight:
- 768
- 256
sequential.6.input_layernorm.bias:
- 256
sequential.6.input_layernorm.weight:
- 256
sequential.6.mlp.dense_4h_to_h.bias:
- 256
sequential.6.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.6.mlp.dense_h_to_4h.bias:
- 1024
sequential.6.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.6.post_attention_layernorm.bias:
- 256
sequential.6.post_attention_layernorm.weight:
- 256
sequential.7.attention.dense.bias:
- 256
sequential.7.attention.dense.weight:
- 256
- 256
sequential.7.attention.query_key_value.bias:
- 768
sequential.7.attention.query_key_value.weight:
- 768
- 256
sequential.7.input_layernorm.bias:
- 256
sequential.7.input_layernorm.weight:
- 256
sequential.7.mlp.dense_4h_to_h.bias:
- 256
sequential.7.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.7.mlp.dense_h_to_4h.bias:
- 1024
sequential.7.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.7.post_attention_layernorm.bias:
- 256
sequential.7.post_attention_layernorm.weight:
- 256
sequential.8.attention.dense.bias:
- 256
sequential.8.attention.dense.weight:
- 256
- 256
sequential.8.attention.query_key_value.bias:
- 768
sequential.8.attention.query_key_value.weight:
- 768
- 256
sequential.8.input_layernorm.bias:
- 256
sequential.8.input_layernorm.weight:
- 256
sequential.8.mlp.dense_4h_to_h.bias:
- 256
sequential.8.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.8.mlp.dense_h_to_4h.bias:
- 1024
sequential.8.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.8.post_attention_layernorm.bias:
- 256
sequential.8.post_attention_layernorm.weight:
- 256
sequential.9.attention.dense.bias:
- 256
sequential.9.attention.dense.weight:
- 256
- 256
sequential.9.attention.query_key_value.bias:
- 768
sequential.9.attention.query_key_value.weight:
- 768
- 256
sequential.9.input_layernorm.bias:
- 256
sequential.9.input_layernorm.weight:
- 256
sequential.9.mlp.dense_4h_to_h.bias:
- 256
sequential.9.mlp.dense_4h_to_h.weight:
- 256
- 1024
sequential.9.mlp.dense_h_to_4h.bias:
- 1024
sequential.9.mlp.dense_h_to_4h.weight:
- 1024
- 256
sequential.9.post_attention_layernorm.bias:
- 256
sequential.9.post_attention_layernorm.weight:
- 256
